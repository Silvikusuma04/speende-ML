# -*- coding: utf-8 -*-
"""loan_approval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10oJ-vVbC3Ul8ZoUhWWdYIK9-H7KTzoSf

# About Dataset

# **Dataset Sintetis untuk Penilaian Risiko dan Pemodelan Persetujuan Pinjaman**
link dataset : https://www.kaggle.com/datasets/lorenzozoppelletto/financial-risk-for-loan-approval?select=Loan.csv

Dataset sintetis ini terdiri dari **20.000 catatan** data pribadi dan keuangan yang dirancang untuk memfasilitasi pengembangan model prediktif untuk penilaian risiko. Dataset ini memiliki dua tujuan utama:

- **Regresi Skor Risiko**: Untuk memprediksi **skor risiko** kontinu yang terkait dengan kemungkinan seseorang gagal bayar atau mengalami ketidakstabilan keuangan.
- **Klasifikasi Biner**: Untuk menentukan hasil **biner** dari persetujuan pinjaman, yang menunjukkan apakah pemohon kemungkinan akan disetujui atau ditolak untuk pinjaman.

Dataset ini mencakup berbagai fitur seperti informasi **demografis**, **riwayat kredit**, **status pekerjaan**, **tingkat pendapatan**, **hutang yang ada**, dan metrik keuangan relevan lainnya, memberikan dasar yang komprehensif untuk analisis berbasis data yang canggih dan pengambilan keputusan.

---

### **Dataset ini mencakup kolom-kolom berikut:**

- **ApplicationDate**: Tanggal aplikasi pinjaman
- **Age**: Usia pemohon
- **AnnualIncome**: Pendapatan tahunan
- **CreditScore**: Skor kelayakan kredit
- **EmploymentStatus**: Status pekerjaan
- **EducationLevel**: Tingkat pendidikan tertinggi
- **Experience**: Pengalaman kerja
- **LoanAmount**: Jumlah pinjaman yang diminta
- **LoanDuration**: Periode pembayaran pinjaman
- **MaritalStatus**: Status pernikahan pemohon
- **NumberOfDependents**: Jumlah tanggungan
- **HomeOwnershipStatus**: Status kepemilikan rumah
- **MonthlyDebtPayments**: Pembayaran utang bulanan
- **CreditCardUtilizationRate**: Persentase penggunaan kartu kredit
- **NumberOfOpenCreditLines**: Jumlah jalur kredit yang aktif
- **NumberOfCreditInquiries**: Jumlah pengecekan kredit
- **DebtToIncomeRatio**: Rasio utang terhadap pendapatan
- **BankruptcyHistory**: Riwayat kebangkrutan
- **LoanPurpose**: Tujuan pinjaman
- **PreviousLoanDefaults**: Gagal bayar pinjaman sebelumnya
- **PaymentHistory**: Riwayat pembayaran
- **LengthOfCreditHistory**: Durasi sejarah kredit
- **SavingsAccountBalance**: Saldo tabungan
- **CheckingAccountBalance**: Saldo rekening giro
- **TotalAssets**: Total aset yang dimiliki
- **TotalLiabilities**: Total utang yang dimiliki
- **MonthlyIncome**: Pendapatan bulanan
- **UtilityBillsPaymentHistory**: Riwayat pembayaran tagihan utilitas
- **JobTenure**: Lama bekerja
- **NetWorth**: Kekayaan bersih
- **BaseInterestRate**: Suku bunga awal
- **InterestRate**: Suku bunga yang diterapkan
- **MonthlyLoanPayment**: Pembayaran pinjaman bulanan
- **TotalDebtToIncomeRatio**: Total utang terhadap pendapatan
- **LoanApproved**: Status persetujuan pinjaman
- **RiskScore**: Skor penilaian risiko

# Import Library
"""

pip install tensorflowjs

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import OrdinalEncoder, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix,classification_report, accuracy_score
from sklearn.feature_selection import mutual_info_classif

from imblearn.over_sampling import SMOTE
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.models import load_model
import tensorflowjs as tfjs

import joblib
import pickle

"""# Load Data

"""

pd.set_option('display.max_columns',None)

df = pd.read_csv('/content/drive/MyDrive/Coding Camp - DBS Fundation/Dataset Capstone/Loan Approval/Loan.csv')
df.head()

df['PreviousLoanDefaults'].unique()

"""# EDA"""

df.info()

df.describe(include='all')

df.isnull().sum()

df.duplicated().sum()

for col in df.columns:
    unique_vals = df[col].value_counts()
    print(f'{col} has {len(unique_vals)} unique values')

# Mengidentifikasi jenis kolom (variabel) dalam sebuah dataframe berdasarkan tipe data dan karakteristiknya

def grab_col_names(dataframe, cat_th = 10, car_th = 20): #  memisahkan kolom dalam dataset berdasarkan tipe dan karakteristiknya.

    cat_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["category", "object", "bool"]] # kolom kategorikal (tipe "category", "object", "bool", atau numerik dengan unique values < cat_th).
    num_but_cat = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["int64", "float64"] and dataframe[col].nunique() < cat_th] # kolom numerik yang bertindak seperti kategorikal (berdasarkan jumlah unique values).
    cat_but_car = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["category", "object"] and dataframe[col].nunique() > car_th] # kolom kategorikal dengan unique values > car_th (high cardinality).

    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # kolom numerik murni (int64, float64) yang bukan kategorikal.
    num_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["int64", "float64"]]
    num_cols = [col for col in num_cols if col not in cat_cols]

    print(f"Jumlah observasi: {dataframe.shape[0]}")
    print(f"Jumlah variabel: {dataframe.shape[1]}")
    print(f"Kolom kategorikal: {len(cat_cols)}")
    print(f"Kolom Numerik: {len(num_cols)}")
    print(f"Kategori tapi kardinal: {len(cat_but_car)}")
    print(f"Numerik tapi kategorikal: {len(num_but_cat)}")

    # mengembalikan daftar kolom yang dikelompokkan berdasarkan tipe (cat_cols, num_cols, cat_but_car).
    return cat_cols, num_cols, cat_but_car

grab_dataset_cols = grab_col_names(df)

# Memvisualisasikan distribusi kolom-kolom kategorikal dalam sebuah dataframe menggunakan dua jenis plot: countplot dan pie plot
def plot_categorical(dataframe, categorical_columns): # membuat visualisasi untuk kolom kategorikal dalam dataset.
    num_cols = len(categorical_columns) # daftar kolom kategorikal yang akan divisualisasikan.
    num_rows = num_cols
    fig, axes = plt.subplots(num_rows, 2, figsize=(20, 5 * num_rows))
    axes = axes.flatten()

    for i, col in enumerate(categorical_columns):
        # Countplot: Membuat countplot untuk distribusi jumlah kategori.
        sns.countplot(x=col, data=dataframe, ax=axes[2*i], hue=col) # menunjukkan jumlah observasi dalam setiap kategori menggunakan bar chart.
        axes[2*i].set_title(f'{col} Count')
        axes[2*i].set_xlabel(col)
        axes[2*i].set_ylabel('Count')

        # Pieplot: Membuat pieplot untuk distribusi proporsi kategori.
        dataframe[col].value_counts().plot.pie(autopct='%1.1f%%', ax=axes[2*i+1]) # menampilkan proporsi kategori dalam bentuk diagram lingkaran.
        axes[2*i+1].set_title(f'{col} Distribution')
        axes[2*i+1].set_ylabel('')

    # Menyesuaikan layout dengan plt.tight_layout() untuk hasil visualisasi yang rapi.
    plt.tight_layout()
    plt.show()

vis_categorical = plot_categorical(df, grab_dataset_cols[0])

# Membuat histogram bagi setiap kolom numerik dalam sebuah dataframe
def plot_histograms(dataframe, numeric_columns): # membuat histogram untuk setiap kolom numerik dalam dataset.

    num_cols = len(numeric_columns) # daftar kolom numerik yang akan divisualisasikan.
    num_rows = (num_cols + 1) // 4 + ((num_cols + 1) % 4 != 0)
    fig, axes = plt.subplots(num_rows, 4, figsize=(20, 5 * num_rows))
    axes = axes.flatten()

    for i, col in enumerate(numeric_columns):
        dataframe[col].hist(ax=axes[i], bins=20)  # Untuk mengurangi ukurannya, kami mengurangi nilai tempat sampah.
        axes[i].set_title(col)
        axes[i].set_xlabel(col)
        axes[i].set_ylabel('Frequency')

    for j in range(num_cols, num_rows * 4):
        fig.delaxes(axes[j])  # Jika jumlah kolom tidak habis dibagi 4, hapus sumbu yang berlebihan

    # menampilkan histogram frekuensi untuk setiap kolom numerik.
    plt.tight_layout()
    plt.show()

vis_numerical = plot_histograms(df, grab_dataset_cols[1])

def identity_outliers(dataframe, q1=0.05, q3=0.95):
    # Filter numeric columns
    numeric_columns = dataframe.select_dtypes(include=['number']).columns

    # Set up the subplot grid for side-by-side visualization
    num_columns = 4  # Number of columns per row for the grid
    num_rows = (len(numeric_columns) + num_columns - 1) // num_columns  # Calculate required number of rows

    fig, axes = plt.subplots(num_rows, num_columns, figsize=(16, num_rows * 5))
    axes = axes.flatten()  # Flatten axes array for easier indexing

    outlier_results = {}

    # Loop through each numeric column to detect outliers and plot
    for idx, col_name in enumerate(numeric_columns):
        # Calculate quartiles and IQR
        quartile1 = dataframe[col_name].quantile(q1)
        quartile3 = dataframe[col_name].quantile(q3)
        iqr = quartile3 - quartile1
        lower_bound = quartile1 - (1.5 * iqr)
        upper_bound = quartile3 + (1.5 * iqr)

        # Identify outliers
        outliers = dataframe[(dataframe[col_name] < lower_bound) | (dataframe[col_name] > upper_bound)]
        outlier_results[col_name] = outliers

        # Plotting boxplot for each column
        sns.boxplot(x=dataframe[col_name], color='skyblue', width=0.5, ax=axes[idx])
        axes[idx].axvline(x=lower_bound, color='red', linestyle='--', label='Lower Bound')
        axes[idx].axvline(x=upper_bound, color='red', linestyle='--', label='Upper Bound')
        axes[idx].set_title(f'Outlier Detection for {col_name}')
        axes[idx].legend()

    # Hide empty subplots if any
    for i in range(len(numeric_columns), len(axes)):
        axes[i].axis('off')

    plt.tight_layout()  # Adjust layout for better spacing between plots
    plt.show()

    return outlier_results

# Example usage
# outliers_all = identity_outliers_all_numeric_grid(dataframe)

outliers = identity_outliers(df)

# Menghitung matriks korelasi
numeric_columns = df.select_dtypes(include=['number']).columns
correlation_matrix = df[numeric_columns].corr()

# Menampilkan korelasi sebagai heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Matriks Korelasi Antar Fitur Numerik')
plt.show()

"""# Preprocessing

## Rename Colom
"""

column_mapping = {
    'ApplicationDate': 'Tanggal_Aplikasi',
    'Age': 'Usia_Pemohon',
    'AnnualIncome': 'Pendapatan_Tahunan',
    'CreditScore': 'Skor_Kelayakan_Kredit',
    'EmploymentStatus': 'Status_Pekerjaan',
    'EducationLevel': 'Tingkat_Pendidikan',
    'Experience': 'Pengalaman_Kerja',
    'LoanAmount': 'Jumlah_Pinjaman',
    'LoanDuration': 'Periode_Pembayaran_Pinjaman',
    'MaritalStatus': 'Status_Pernikahan_Pemohon',
    'NumberOfDependents': 'Jumlah_Tanggungan',
    'HomeOwnershipStatus': 'Status_Kepemilikan_Rumah',
    'MonthlyDebtPayments': 'Pembayaran_Utang_Bulanan',
    'CreditCardUtilizationRate': 'Persentase_Penggunaan_Kartu_Kredit',
    'NumberOfOpenCreditLines': 'Jumlah_Jalur_Kredit_Aktif',
    'NumberOfCreditInquiries': 'Jumlah_Pengecekan_Kredit',
    'DebtToIncomeRatio': 'Rasio_Utang_Terhadap_Pendapatan',
    'BankruptcyHistory': 'Riwayat_Kebangkrutan',
    'LoanPurpose': 'Tujuan_Pinjaman',
    'PreviousLoanDefaults': 'Gagal_Bayar_Pinjaman_Sebelumnya',
    'PaymentHistory': 'Riwayat_Pembayaran',
    'LengthOfCreditHistory': 'Durasi_Sejarah_Kredit',
    'SavingsAccountBalance': 'Saldo_Tabungan',
    'CheckingAccountBalance': 'Saldo_Rekening_Giro',
    'TotalAssets': 'Total_Aset',
    'TotalLiabilities': 'Total_Kewajiban',
    'MonthlyIncome': 'Pendapatan_Bulanan',
    'UtilityBillsPaymentHistory': 'Riwayat_Pembayaran_Tagihan_Utilitas',
    'JobTenure': 'Lama_Bekerja',
    'NetWorth': 'Kekayaan_Bersih',
    'BaseInterestRate': 'Suku_Bunga_Awal',
    'InterestRate': 'Suku_Bunga_Yang_Diterapkan',
    'MonthlyLoanPayment': 'Pembayaran_Pinjaman_Bulanan',
    'TotalDebtToIncomeRatio': 'Total_Utang_Terhadap_Pendapatan',
    'LoanApproved': 'Status_Persetujuan_Pinjaman',
    'RiskScore': 'Skor_Penilaian_Risiko'
}

# Apply the column mapping
df.rename(columns=column_mapping, inplace=True)

df.info()

df.isnull().sum()

df.duplicated().sum()

object_cols = [col for col in df.columns if str(df[col].dtypes) in ["category", "object"]]
object_cols

# Iterate through the list of object column names
for col in object_cols:
    # Print the unique values for each column
    print(f"Unique values for column '{col}':")
    print(df[col].unique())
    print("-" * 30) # Print a separator for readability

"""## Convert to IDR"""

exchange_rate = 16.500

df['Jumlah_Pinjaman'] = df['Jumlah_Pinjaman'] * exchange_rate
df['Pendapatan_Bulanan'] = df['Pendapatan_Bulanan'] * exchange_rate
df['Pendapatan_Tahunan'] = df['Pendapatan_Tahunan'] * exchange_rate
df['Total_Aset'] = df['Total_Aset'] * exchange_rate
df['Saldo_Tabungan'] = df['Saldo_Tabungan'] * exchange_rate
df['Saldo_Rekening_Giro'] = df['Saldo_Rekening_Giro'] * exchange_rate
df['Total_Kewajiban'] = df['Total_Kewajiban'] * exchange_rate
df['Kekayaan_Bersih'] = df['Kekayaan_Bersih'] * exchange_rate
df['Pembayaran_Pinjaman_Bulanan'] = df['Pembayaran_Pinjaman_Bulanan'] * exchange_rate
df['Pembayaran_Utang_Bulanan'] = df['Pembayaran_Utang_Bulanan'] * exchange_rate

df.head()

"""## Mapping Education Level"""

df['Tingkat_Pendidikan'].unique()

df['Tingkat_Pendidikan'] = df['Tingkat_Pendidikan'].map({'High School':'SMA', 'Associate': 'Diploma', 'Bachelor': 'Sarjana', 'Master': 'Magister', 'Doctorate': 'Doktor'})

df['Tingkat_Pendidikan'].unique()

"""## Mapping Employment"""

df['Status_Pekerjaan'].unique()

df['Status_Pekerjaan'] = df['Status_Pekerjaan'].map({'Employed': 'Pegawai', 'Self-Employed': 'Wiraswasta', 'Unemployed': 'Tidak Bekerja'})

df['Status_Pekerjaan'].unique()

"""## Mapping MaritalStatus"""

df['Status_Pernikahan_Pemohon'].unique()

df['Status_Pernikahan_Pemohon'] = df['Status_Pernikahan_Pemohon'].map({'Married': 'Menikah', 'Single': 'Single', 'Divorced': 'Cerai', 'Widowed': 'Janda'})

df['Status_Pernikahan_Pemohon'].unique()

"""## Mapping HomeOwnershipStatus"""

df['Status_Kepemilikan_Rumah'].unique()

df['Status_Kepemilikan_Rumah'] = df['Status_Kepemilikan_Rumah'].map({'Own': 'Sendiri', 'Rent': 'Sewa', 'Mortgage': 'Pinjaman', 'Other': 'Lainnya'})

df['Status_Kepemilikan_Rumah'].unique()

"""## Mapping LoanPurpose"""

df['Tujuan_Pinjaman'].unique()

df['Tujuan_Pinjaman'] = df['Tujuan_Pinjaman'].map({'Home': 'Rumah', 'Debt Consolidation': 'Bisnis', 'Education': 'Pendidikan', 'Other': 'Lainnya', 'Auto': 'Kendaraan'})

df['Tujuan_Pinjaman'].unique()

"""## Encoding"""

le = LabelEncoder()

education_order = ['SMA',  'Diploma' , 'Sarjana' , 'Magister' , 'Doktor']
ordinal_encoder = OrdinalEncoder(categories=[education_order])

df['Tingkat_Pendidikan'] = ordinal_encoder.fit_transform(df[['Tingkat_Pendidikan']])
df['Status_Pernikahan_Pemohon'] = le.fit_transform(df['Status_Pernikahan_Pemohon'])
df['Status_Kepemilikan_Rumah'] = le.fit_transform(df['Status_Kepemilikan_Rumah'])
df['Status_Pekerjaan'] = le.fit_transform(df['Status_Pekerjaan'])
df['Tujuan_Pinjaman'] = le.fit_transform(df['Tujuan_Pinjaman'])

df.head()

joblib.dump(ordinal_encoder, 'ordinal_encoder.pkl')
joblib.dump(le, 'label_encoder.pkl')

correlation = df.drop(['Tanggal_Aplikasi', 'Skor_Penilaian_Risiko'], axis=1).corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Matriks Korelasi Antar Fitur Numerik')
plt.show()

"""# Modelling"""

# Pisahkan fitur dan target
X = df.drop(['Status_Persetujuan_Pinjaman','Tanggal_Aplikasi'], axis=1)
y = df['Status_Persetujuan_Pinjaman']

X.head()

"""## Mutual Information"""

# Standarisasi fitur
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Hitung mutual information antara fitur dan target
mi = mutual_info_classif(X_scaled, y)

# Tampilkan fitur yang memiliki informasi mutual tertinggi
mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)
top_features_mi = mi_series.head(21)

print(top_features_mi)

new_columns = top_features_mi.index.tolist()
X = X[new_columns]

X.head()

X = X.drop('Skor_Penilaian_Risiko',axis = 1)

X.head()

"""## Standarisasi & SMOTE"""

X_scaled = scaler.fit_transform(X)

smote = SMOTE(sampling_strategy='auto')
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

joblib.dump(scaler, 'scaler.pkl')

y_resampled.value_counts()

# Split data menjadi data training dan testing
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Membangun model deep learning dengan regulasi L2 dan dropout
model = Sequential()
# Input layer dan hidden layer pertama dengan L2 regulasi dan dropout
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.1)))
model.add(Dropout(0.3))  # Dropout 30% dari neuron di layer ini

# Hidden layer kedua dengan L2 regulasi dan dropout
model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.4))

# Hidden layer ketiga dengan L2 regulasi dan dropout
model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))

# Output layer untuk klasifikasi biner (LoanApproved)
model.add(Dense(1, activation='sigmoid'))

# Early stopping untuk menghentikan pelatihan jika tidak ada perbaikan pada validation loss
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Kompilasi model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Melatih model dengan early stopping
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=early_stopping)

# Evaluasi model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Model accuracy on test data: {accuracy*100:.2f}%')

model.save('model_loan.h5')

# Plotting Loss
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss during training')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plotting Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy during training')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

"""## Evaluasi Model"""

# Prediksi pada data uji
y_pred = model.predict(X_test)
y_pred_class = (y_pred > 0.5).astype(int)  # Mengkonversi prediksi ke kelas biner (0 atau 1)

# Hitung confusion matrix
cm = confusion_matrix(y_test, y_pred_class)

print(classification_report(y_test, y_pred_class))

# Visualisasikan confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Approved', 'Approved'], yticklabels=['Not Approved', 'Approved'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""# Konversi Model"""

# Simpan model dalam format SavedModel
model.export('model_loan')

# Konversi model dari SavedModel
tflite_converter = tf.lite.TFLiteConverter.from_saved_model('/content/model_loan')
tflite_model = tflite_converter.convert()

with open('model_loan.tflite', 'wb') as f:
    f.write(tflite_model)

tflite_converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = tflite_converter.convert()

with open('model_loan.tflite', 'wb') as f:
    f.write(tflite_model)

tfjs.converters.save_keras_model(model, 'model_loan_tfjs')

!zip -r /content/model_loan_tfjs.zip /content/model_loan_tfjs

!zip -r /content/model_loan.zip /content/model_loan

from google.colab import files
files.download('/content/model_loan_tfjs.zip')
files.download('/content/model_loan.zip')

"""# Inference Model

"""

# Memuat scaler dan encoder dari file pickle
with open('scaler.pkl', 'rb') as file:
    scaler = joblib.load(file)

with open('label_encoder.pkl', 'rb') as file:
    le = joblib.load(file)

with open('ordinal_encoder.pkl', 'rb') as file:
    oe = joblib.load(file)

# Memuat model H5 yang telah disimpan
model = load_model('model_loan.h5')

Total_Utang_Terhadap_Pendapatan    0.231755
Pendapatan_Bulanan                 0.205647
Pendapatan_Tahunan                 0.203119
Suku_Bunga_Yang_Diterapkan         0.055673
Jumlah_Pinjaman                    0.039039
Suku_Bunga_Awal                    0.033682
Tingkat_Pendidikan                 0.024572
Kekayaan_Bersih                    0.023374
Pembayaran_Pinjaman_Bulanan        0.022546
Total_Aset                         0.013556
Usia_Pemohon                       0.013175
Skor_Kelayakan_Kredit              0.012981
Pengalaman_Kerja                   0.009654
Durasi_Sejarah_Kredit              0.007202
Periode_Pembayaran_Pinjaman        0.006929
Pembayaran_Utang_Bulanan           0.004332
Saldo_Tabungan                     0.004051
Jumlah_Pengecekan_Kredit           0.003789
Jumlah_Tanggungan                  0.002668
Jumlah_Jalur_Kredit_Aktif          0.002392

new_data = pd.DataFrame([{'Total_Utang_Terhadap_Pendapatan': 0.17,
                          'Pendapatan_Bulanan': 2000000,
                          'Pendapatan_Tahunan':10000000,
                          'Suku_Bunga_Yang_Diterapkan': 0.3,
                          'Jumlah_Pinjaman': 10000000,
                          'Suku_Bunga_Awal':0.2,
                          'Tingkat_Pendidikan':'Sarjana',
                          'Kekayaan_Bersih': 4000000,
                          'Pembayaran_Pinjaman_Bulanan': 250000,
                          'Total_Aset':10000000,
                          'Usia_Pemohon': 32,
                          'Skor_Kelayakan_Kredit': 5,
                          'Pengalaman_Kerja': 10,
                          'Durasi_Sejarah_Kredit':2,
                          'Periode_Pembayaran_Pinjaman':48,
                          'Pembayaran_Utang_Bulanan':100000,
                          'Saldo_Tabungan':2500000,
                          'Jumlah_Pengecekan_Kredit':1,
                          'Jumlah_Tanggungan':1,
                          'Jumlah_Jalur_Kredit_Aktif':0,
                          }])

new_data['Tingkat_Pendidikan'] = oe.transform(new_data[['Tingkat_Pendidikan']])

new_data_scaled = scaler.transform(new_data)




# Melakukan prediksi
prediksi_proba = model.predict(new_data_scaled)  # Probabilitas (antara 0 dan 1)

# Konversi probabilitas ke label biner (0 atau 1)
prediksi_label = (prediksi_proba > 0.5).astype(int)

# Menampilkan hasil prediksi
print(f"Prediksi probabilitas LoanApproved: {prediksi_proba[0][0]:.4f}")
print(f"Prediksi kelas LoanApproved: {'Approved' if prediksi_label[0][0] == 1 else 'Not Approved'}")